# Sentiment_Analysis_of_Toxic_Comments
Discussing things you care about can be difficult on online social platforms. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. The performance of a model for predicting such toxic comments depends on the distribution of data used to train the model. In this notebook, a multi-headed bidirectional LSTM RNN model is developed that’s capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate. The model is trained and validated using a dataset of comments from Wikipedia’s talk page edits. The labeled dataset is available on <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data">kaggle</a>. Moreover, I use the pretrained GloVe word vectors from <a href="http://nlp.stanford.edu/data/glove.6B.zip">Stanford</a>. 
